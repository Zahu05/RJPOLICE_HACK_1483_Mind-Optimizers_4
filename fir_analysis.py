# -*- coding: utf-8 -*-
"""FIR_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wdDBbmpPjyR_TIHOR1M8fj3lfYLU0lq3
"""

!pip install PyMuPDF python-docx
!sudo apt-get install tesseract-ocr #to extract text from image and enable text based search
!pip install pytesseract pdf2image #Pytesseract acts as a translator between your Python program and Tesseract.
!sudo apt-get install tesseract-ocr
!pip install pytesseract pdf2image PyMuPDF #extract the pictures from that PDF
!pip install PyPDF2 #A library for reading and manipulating PDF files in Python.
!pip install transformers
!pip install chardet # It helps your computer figure out the language or encoding used in a text file.
!pip install matplotlib-venn
!pip install transformers[torch]
!pip install accelerate==0.20.1 #make training your machine learning models faster by spreading the work across multiple devices or computers.
!pip install accelerate -U #accelerate -U is like a command to update or upgrade your accelerate tool

import cv2
import pytesseract
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
import torch
torch.cuda.empty_cache()
from torch.utils.data import DataLoader, Dataset
# Model 1: OCR Model
def extract_text_from_file(file_path):
    # Extract text from different types of files

    # Image files (png, jpg)
    if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):
        img = Image.open(file_path)
        return pytesseract.image_to_string(img) #CONVERT IMAGE TO TEXT

    # Text files (txt)
    elif file_path.lower().endswith('.txt'):
        with open(file_path, 'r', encoding='utf-8') as txt_file:
            return txt_file.read()

    # PDF files
    elif file_path.lower().endswith('.pdf'):
        with open(file_path, 'rb') as pdf_file:
            pdf_reader = PyPDF2.PdfFileReader(pdf_file)
            text = ''
            for page_num in range(pdf_reader.numPages):
                text += pdf_reader.getPage(page_num).extractText()
            return text

    # DOCX files
    elif file_path.lower().endswith('.docx'):
        doc = docx.Document(file_path)
        return '\n'.join([paragraph.text for paragraph in doc.paragraphs])

    else:
        return "Unsupported file format. Please provide an image (png, jpg), text (txt), PDF, or DOCX file."

def perform_ocr_on_file(file_path):

    # Path to the Tesseract executable (change it based on your Colab environment)
    pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

    # Extracting text from the file
    text = extract_text_from_file(file_path)

    # Printing the extracted text
    print("Extracted Text:\n", text)
    return ''.join(text)

# Model 2: IPC_SECTION Prediction Model
# Loading the dataset
import chardet

with open('FIR1.csv', 'rb') as f: #opens the file in binary mode(file is treated as sequenece of bytes rather than character)
    result = chardet.detect(f.read())

dataset = pd.read_csv('FIR1.csv', encoding=result['encoding'])

# Converting string labels to numerical labels using LabelEncoder
label_encoder = LabelEncoder()
dataset['IPC-Section'] = dataset['IPC-Section'].apply(lambda x: x.split(','))

# Splitting the dataset into training and testing sets
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)

# Tokenizing the input data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_tokens = tokenizer(train_data['faults'].tolist(), train_data['complaints'].tolist(), padding=True, truncation=True, return_tensors='pt')# TRUNCATE:sequences longer than the maximum allowed length should be truncated to fit the model's input size.
test_tokens = tokenizer(test_data['faults'].tolist(), test_data['complaints'].tolist(), padding=True, truncation=True, return_tensors='pt')

# Creating PyTorch datasets
class CrimeDataset(Dataset):
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {'input_ids': self.tokens['input_ids'][idx],
                'attention_mask': self.tokens['attention_mask'][idx],
                'labels': torch.tensor(self.labels[idx])}  # Adjust to handle multiple labels

# Ensuring the labels are converted to a list for training and testing
train_labels = train_data['IPC_SECTION'].tolist()
test_labels = test_data['IPC_SECTION'].tolist()

# Using the original labels while creating datasets
train_dataset = CrimeDataset(train_tokens, train_labels)
test_dataset = CrimeDataset(test_tokens, test_labels)

# Loading pre-trained BERT model for sequence classification
#bert-base-uncased:the sentence is converted into lowercase
ipc_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(dataset['IPC_SECTION'].unique()))

# Set up optimizer
ipc_optimizer = AdamW(ipc_model.parameters(), lr=0.0002, no_deprecation_warning=True)

# Set up data loaders
train_loader = DataLoader(train_dataset, batch_size=30, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=30, shuffle=False)

# Train the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ipc_model.to(device)
num_epochs = 55  # Adjust the number of epochs as needed

# Set up the learning rate scheduler
#the learning rate will be updated after every 2 epoch
#the gamma parameter controls how much the learning rate is multiplied by at each learning rate update
scheduler = torch.optim.lr_scheduler.StepLR(ipc_optimizer, step_size=2, gamma=0.9) #the step size in a learning rate scheduler provides a means to control the rate at which the learning rate changes during training.

# Training the model
# Inside the training loop
accumulation_steps = 2 #THESE are used when we have limited memory and we have to work with small batch size and after each accumulation he gradients are update
counter = 0  # Counter to track accumulation steps

for epoch in range(num_epochs):
    ipc_model.train()
    total_loss = 0.0

    for batch in train_loader:
        inputs = {k: v.to(device) for k, v in batch.items()}
        outputs = ipc_model(**inputs)
        loss = outputs.loss
        loss.backward() #backward pass:to check if the prediction made by the model is right or wrong.if wrong then adjust the gradients
        torch.cuda.set_per_process_memory_fraction(0.5)

        counter += 1
        #gradient stores the info about how output should be changed inorder to minimize the loss fucntion
        if counter % accumulation_steps == 0: #if our counter reaches the accumulation step then update
            ipc_optimizer.step() #to update the model's parameter(adamW in our case is updated)
            ipc_optimizer.zero_grad() #set gradient to zero inorder to clear batches otherwise wrong gradient info

        total_loss += loss.item() #keeps the loss of each batch

    # Calculate average training loss
    avg_train_loss = total_loss / len(train_loader)

    # Print or log the training loss
    print(f"Epoch {epoch + 1}/{num_epochs}, Avg. Training Loss: {avg_train_loss:.4f}")

    # Evaluate the model on the test set
    ipc_model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():
        for batch in test_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = ipc_model(**inputs) #contains the model's prediction
            logits = outputs.logits #output before applying any activation fucntion(represents unnnormalized prediction)
            predictions.extend(logits.argmax(dim=1).cpu().numpy()) #It collects the model's predictions for each example in the test set.
            true_labels.extend(batch['labels'].cpu().numpy()) #"true labels" refer to the actual or correct class labels associated with a set of examples in a dataset

    # Calculating accuracy
    accuracy = accuracy_score(true_labels, predictions) #the true labels and predicted ones are comapred to calculate the accuracy
    print(f"Epoch {epoch + 1}/{num_epochs}, Model Accuracy: {accuracy * 100:.2f}%")
    print("True Labels:", true_labels)
    print("Predictions:", predictions)

# Now you can use the trained model to predict IPC_SECTION for new descriptions
ipdata = perform_ocr_on_file('temp.txt')
print("\n")
new_description = ipdata

# Tokenize the new description
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
new_tokens = tokenizer(new_description, padding=True, truncation=True, return_tensors='pt')

# Perform prediction using your model
new_outputs = ipc_model(**new_tokens)

# Get predicted probabilities
probabilities = torch.sigmoid(new_outputs.logits)

# Assuming 'threshold' as a probability cutoff for considering a label
threshold = 0.5

# Get indices where probabilities are above the threshold for multi-labels
predicted_indices = (probabilities > threshold).nonzero().squeeze()

# Map the predicted indices to the original labels using inverse transformation
predicted_labels = label_encoder.inverse_transform(predicted_indices)
print(f"Predicted Sections: {predicted_labels}")